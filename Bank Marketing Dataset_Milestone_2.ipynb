{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a535bf1-19f5-4c89-9694-07e6fc5e848e",
   "metadata": {},
   "source": [
    "## Milestone 2: Predictive Modeling\n",
    "In this milestone, we will fully integrate the data and process it for machine learning. Below is a refined step-by-step plan for predictive modeling, including detailed descriptions and code for each step.\n",
    "\n",
    "### 1. Load and Inspect the Data\n",
    "#### What We'll Do\n",
    "We will load the files into DataFrames and inspect their structure. This helps verify that features (df_X.csv) and the target variable (df_y.csv) are correctly aligned. We'll also reference the info.csv metadata to guide preprocessing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de84791c-d1ae-4a96-81f9-d201aa03275c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  age           job  marital  education default  balance housing  \\\n",
      "0           0   58    management  married   tertiary      no     2143     yes   \n",
      "1           1   44    technician   single  secondary      no       29     yes   \n",
      "2           2   33  entrepreneur  married  secondary      no        2     yes   \n",
      "3           3   47   blue-collar  married        NaN      no     1506     yes   \n",
      "4           4   33           NaN   single        NaN      no        1      no   \n",
      "\n",
      "  loan contact  day_of_week month  duration  campaign  pdays  previous  \\\n",
      "0   no     NaN            5   may       261         1     -1         0   \n",
      "1   no     NaN            5   may       151         1     -1         0   \n",
      "2  yes     NaN            5   may        76         1     -1         0   \n",
      "3   no     NaN            5   may        92         1     -1         0   \n",
      "4   no     NaN            5   may       198         1     -1         0   \n",
      "\n",
      "  poutcome  \n",
      "0      NaN  \n",
      "1      NaN  \n",
      "2      NaN  \n",
      "3      NaN  \n",
      "4      NaN  \n",
      "   Unnamed: 0   y\n",
      "0           0  no\n",
      "1           1  no\n",
      "2           2  no\n",
      "3           3  no\n",
      "4           4  no\n",
      "   Unnamed: 0       name     role         type      demographic  \\\n",
      "0           0        age  Feature      Integer              Age   \n",
      "1           1        job  Feature  Categorical       Occupation   \n",
      "2           2    marital  Feature  Categorical   Marital Status   \n",
      "3           3  education  Feature  Categorical  Education Level   \n",
      "4           4    default  Feature       Binary              NaN   \n",
      "\n",
      "                                         description units missing_values  \n",
      "0                                                NaN   NaN             no  \n",
      "1  type of job (categorical: 'admin.','blue-colla...   NaN             no  \n",
      "2  marital status (categorical: 'divorced','marri...   NaN             no  \n",
      "3  (categorical: 'basic.4y','basic.6y','basic.9y'...   NaN             no  \n",
      "4                             has credit in default?   NaN             no  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 17 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Unnamed: 0   45211 non-null  int64 \n",
      " 1   age          45211 non-null  int64 \n",
      " 2   job          44923 non-null  object\n",
      " 3   marital      45211 non-null  object\n",
      " 4   education    43354 non-null  object\n",
      " 5   default      45211 non-null  object\n",
      " 6   balance      45211 non-null  int64 \n",
      " 7   housing      45211 non-null  object\n",
      " 8   loan         45211 non-null  object\n",
      " 9   contact      32191 non-null  object\n",
      " 10  day_of_week  45211 non-null  int64 \n",
      " 11  month        45211 non-null  object\n",
      " 12  duration     45211 non-null  int64 \n",
      " 13  campaign     45211 non-null  int64 \n",
      " 14  pdays        45211 non-null  int64 \n",
      " 15  previous     45211 non-null  int64 \n",
      " 16  poutcome     8252 non-null   object\n",
      "dtypes: int64(8), object(9)\n",
      "memory usage: 5.9+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45211 entries, 0 to 45210\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  45211 non-null  int64 \n",
      " 1   y           45211 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 706.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load features and target variable\n",
    "df_X = pd.read_csv(\"df_X.csv\")  # Replace with correct path\n",
    "df_y = pd.read_csv(\"df_y.csv\")  # Replace with correct path\n",
    "info = pd.read_csv(\"info.csv\")  # Metadata for understanding features\n",
    "\n",
    "# Inspect the data\n",
    "print(df_X.head())  # First rows of features\n",
    "print(df_y.head())  # First rows of target variable\n",
    "print(info.head())  # Feature metadata\n",
    "print(df_X.info())  # Summary of df_X\n",
    "print(df_y.info())  # Summary of df_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030ed091-2af4-4dec-bdbf-862800750063",
   "metadata": {},
   "source": [
    "### 2. Merge Features and Target Variable\n",
    "#### What We'll Do\n",
    "We will combine df_X and df_y into a single DataFrame. This ensures the target variable (y) is aligned with its respective features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb30b596-515e-4132-a7cc-61c30df79dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  age           job  marital  education default  balance housing  \\\n",
      "0           0   58    management  married   tertiary      no     2143     yes   \n",
      "1           1   44    technician   single  secondary      no       29     yes   \n",
      "2           2   33  entrepreneur  married  secondary      no        2     yes   \n",
      "3           3   47   blue-collar  married        NaN      no     1506     yes   \n",
      "4           4   33           NaN   single        NaN      no        1      no   \n",
      "\n",
      "  loan contact  day_of_week month  duration  campaign  pdays  previous  \\\n",
      "0   no     NaN            5   may       261         1     -1         0   \n",
      "1   no     NaN            5   may       151         1     -1         0   \n",
      "2  yes     NaN            5   may        76         1     -1         0   \n",
      "3   no     NaN            5   may        92         1     -1         0   \n",
      "4   no     NaN            5   may       198         1     -1         0   \n",
      "\n",
      "  poutcome  \n",
      "0      NaN  \n",
      "1      NaN  \n",
      "2      NaN  \n",
      "3      NaN  \n",
      "4      NaN  \n",
      "Index(['Unnamed: 0', 'age', 'job', 'marital', 'education', 'default',\n",
      "       'balance', 'housing', 'loan', 'contact', 'day_of_week', 'month',\n",
      "       'duration', 'campaign', 'pdays', 'previous', 'poutcome'],\n",
      "      dtype='object')\n",
      "   Unnamed: 0   y\n",
      "0           0  no\n",
      "1           1  no\n",
      "2           2  no\n",
      "3           3  no\n",
      "4           4  no\n",
      "Index(['Unnamed: 0', 'y'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Inspect df_X\n",
    "print(df_X.head())\n",
    "print(df_X.columns)\n",
    "\n",
    "# Inspect df_y\n",
    "print(df_y.head())\n",
    "print(df_y.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f9d84c-200b-422e-b785-244d3272841c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45211, 17)\n",
      "(45211, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load df_X and df_y\n",
    "df_X = pd.read_csv(\"df_X.csv\")\n",
    "df_y = pd.read_csv(\"df_y.csv\")\n",
    "\n",
    "# Confirm DataFrames are loaded correctly\n",
    "print(df_X.shape)  # Should show non-zero rows and columns\n",
    "print(df_y.shape)  # Should show non-zero rows and columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1c90678-78c9-4647-a3a0-3db5dace816c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45211, 18)\n",
      "   Unnamed: 0  age           job  marital  education default  balance housing  \\\n",
      "0           0   58    management  married   tertiary      no     2143     yes   \n",
      "1           1   44    technician   single  secondary      no       29     yes   \n",
      "2           2   33  entrepreneur  married  secondary      no        2     yes   \n",
      "3           3   47   blue-collar  married        NaN      no     1506     yes   \n",
      "4           4   33           NaN   single        NaN      no        1      no   \n",
      "\n",
      "  loan contact  day_of_week month  duration  campaign  pdays  previous  \\\n",
      "0   no     NaN            5   may       261         1     -1         0   \n",
      "1   no     NaN            5   may       151         1     -1         0   \n",
      "2  yes     NaN            5   may        76         1     -1         0   \n",
      "3   no     NaN            5   may        92         1     -1         0   \n",
      "4   no     NaN            5   may       198         1     -1         0   \n",
      "\n",
      "  poutcome   y  \n",
      "0      NaN  no  \n",
      "1      NaN  no  \n",
      "2      NaN  no  \n",
      "3      NaN  no  \n",
      "4      NaN  no  \n"
     ]
    }
   ],
   "source": [
    "# Merge DataFrames on Unnamed: 0\n",
    "df_X = df_X.merge(df_y, on='Unnamed: 0', how='inner')\n",
    "\n",
    "# Inspect the merged DataFrame\n",
    "print(df_X.shape)  # Check the number of rows and columns after the merge\n",
    "print(df_X.head())  # Display the first few rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd7662-9fbb-49a2-803d-fd485d36bc50",
   "metadata": {},
   "source": [
    "### 3. Preprocess the Data\n",
    "#### What We'll Do:\n",
    "\n",
    "**1. Convert the target variable (y) to binary format** (no = 0, yes = 1).\n",
    "\n",
    "**2. Handle Missing Values:**\n",
    "\n",
    "- Replace missing values in contact and poutcome with \"unknown\".\n",
    "\n",
    "- Impute missing values in pdays with -1, which indicates \"not previously contacted.\"\n",
    "\n",
    "**3. Exclude Irrelevant Features:**\n",
    "\n",
    "- Drop the duration column as it is not suitable for realistic predictions.\n",
    "\n",
    "**4. Separate Numerical and Categorical Features:**\n",
    "\n",
    "- Divide the dataset into numerical (e.g., age, balance) and categorical (e.g., job, education) features for more efficient processing.\n",
    "\n",
    "Why? Proper preprocessing ensures the dataset is clean, consistent, and suitable for machine learning models. Categorical features need to be encoded, and numerical features need to be scaled for accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb9c819-4e49-46b7-a170-717827304704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "'duration' column dropped.\n"
     ]
    }
   ],
   "source": [
    "# Convert 'y' to binary format (no=0, yes=1)\n",
    "df_X['y'] = df_X['y'].map({'no': 0, 'yes': 1})\n",
    "print(df_X['y'].unique())  # Verify 'y' contains only [0, 1]\n",
    "\n",
    "# Handle missing values\n",
    "df_X['contact'] = df_X['contact'].fillna(\"unknown\")  # Fill missing 'contact' with \"unknown\"\n",
    "df_X['poutcome'] = df_X['poutcome'].fillna(\"unknown\")  # Fill missing 'poutcome' with \"unknown\"\n",
    "df_X['pdays'] = df_X['pdays'].fillna(-1)  # Replace missing 'pdays' with -1\n",
    "\n",
    "# Check if 'duration' exists before dropping it\n",
    "if 'duration' in df_X.columns:\n",
    "    df_X.drop(columns=['duration'], inplace=True)\n",
    "    print(\"'duration' column dropped.\")\n",
    "else:\n",
    "    print(\"'duration' column not found in the dataset.\")\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "numerical_features = ['age', 'balance', 'campaign', 'pdays', 'previous']\n",
    "categorical_features = ['job', 'marital', 'education', 'contact', 'month', 'poutcome']\n",
    "\n",
    "# Extract numerical and categorical features\n",
    "X_num = df_X[numerical_features]\n",
    "X_cat = df_X[categorical_features]\n",
    "y = df_X['y']  # Extract target variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9b84f8-9d79-484c-bb63-c22acd9063db",
   "metadata": {},
   "source": [
    "### 4. Transform Features\n",
    "#### What We'll Do\n",
    "1. **Scale Numerical Features:**\n",
    "\n",
    "- Use StandardScaler to standardize numerical data for consistent scales.\n",
    "\n",
    "2. **Encode Categorical Features:**\n",
    "\n",
    "- Convert categorical data into numerical format using one-hot encoding.\n",
    "\n",
    "3. **Combine Processed Features:**\n",
    "\n",
    "- Merge scaled numerical features and encoded categorical features into a single feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdb32328-27e7-4c2d-b941-1b8d1fec0ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_final: (45211, 37)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  # Updated to avoid deprecated 'sparse'\n",
    "X_cat_encoded = encoder.fit_transform(X_cat)\n",
    "\n",
    "# Combine scaled numerical and encoded categorical features\n",
    "X_final = np.concatenate([X_num_scaled, X_cat_encoded], axis=1)\n",
    "\n",
    "# Verify the shape of the final dataset\n",
    "print(\"Shape of X_final:\", X_final.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ea9698-adfd-4cb5-98b4-2c2d2cdf0c43",
   "metadata": {},
   "source": [
    "### 5. Split the Data\n",
    "#### What We'll Do\n",
    "Divide the dataset into training (80%) and testing (20%) sets to evaluate model performance on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5b2d6fa-c7b2-4632-9a18-8b2cf1c28000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (36168, 37)\n",
      "X_test shape: (9043, 37)\n",
      "y_train shape: (36168,)\n",
      "y_test shape: (9043,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify the sizes of the splits\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c19da-3ff4-4311-ada7-f55507b19ae0",
   "metadata": {},
   "source": [
    "#### What the Code Does\n",
    "1. train_test_split():\n",
    "\n",
    "- Takes the full dataset (X_final and y) and splits it into X_train, X_test, y_train, and y_test.\n",
    "\n",
    "- test_size=0.2 means 20% of the data is allocated for testing.\n",
    "\n",
    "2. random_state=42:\n",
    "\n",
    "- Ensures reproducibility. Using the same random seed will result in consistent splits every time.\n",
    "\n",
    "3. Shapes of the Splits:\n",
    "\n",
    "- Print the shapes to confirm the data is split correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512b4096-94ff-4083-b9a0-e9bb655eb7ee",
   "metadata": {},
   "source": [
    "### 6. Train Machine Learning Models\n",
    "#### Objective\n",
    "In this step, we’ll train various machine learning models to predict whether a client will subscribe to a term deposit (y) based on the preprocessed features. We’ll implement multiple models to compare their performance in subsequent steps.\n",
    "\n",
    "#### Models to Train\n",
    "\n",
    "1. **Logistic Regression**: A simple, interpretable baseline model for binary classification.\n",
    "\n",
    "2. **Random Forest**: A robust non-linear model that handles complex relationships well.\n",
    "\n",
    "3. **Gradient Boosting (e.g., XGBoost)**: An advanced boosting model for high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc49ac00-eecf-42f2-ad99-30457ed2208b",
   "metadata": {},
   "source": [
    "##### 6.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63bf233d-87ef-4657-8511-2f2848089666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_lr = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3e1e8d-0f1c-4317-bb74-47eddfbc26cc",
   "metadata": {},
   "source": [
    "##### 6.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de6b3b55-4a8d-41db-b8af-9cf74a4534cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f6604-d556-4836-b5bd-241a9b1a5ba9",
   "metadata": {},
   "source": [
    "##### 6.3 Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "defd1563-c109-46a4-ac6e-ee591496ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Train Gradient Boosting (XGBoost)\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_xgb = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c41bb46-2be4-4a9c-889a-8ace3c57a798",
   "metadata": {},
   "source": [
    "#### What This Code Does:\n",
    "1. Initializes each model with default hyperparameters.\n",
    "\n",
    "2. Fits each model to the training data (X_train, y_train).\n",
    "\n",
    "3. Makes predictions on the test set (X_test)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2418d6ec-dabb-435a-8ad5-7e41cef5fed6",
   "metadata": {},
   "source": [
    "### 7: Perform Basic Hyperparameter Tuning\n",
    "#### Objective\n",
    "To improve model performance by adjusting key parameters like:\n",
    "\n",
    "1. **Tree Depth (max_depth)**: Controls the maximum depth of the decision trees.\n",
    "\n",
    "2. **Number of Trees (n_estimators)**: Determines how many trees the model will build.\n",
    "\n",
    "3. **Learning Rate (learning_rate)**: Applies to gradient boosting models (e.g., XGBoost) and controls the contribution of each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3a6e22-743b-4312-9598-5065f0cffe87",
   "metadata": {},
   "source": [
    "#### 7.1 Hyperparameter Tuning for Random Forest\n",
    "We’ll perform basic tuning for parameters like n_estimators (number of trees) and max_depth (tree depth):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7dae67c-b4c4-4ddb-b357-7fab8fdd8219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best Parameters for Random Forest: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Best Score for Random Forest: 0.8948517303556132\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300], # Number of trees\n",
    "    'max_depth': [10, 20, None], # Tree depth\n",
    "    'min_samples_split': [2, 5, 10] # Minimum samples to split a node\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "print(\"Best Score for Random Forest:\", grid_search_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e4514-c582-4055-9e78-9474de60440e",
   "metadata": {},
   "source": [
    "#### 7.2 Hyperparameter Tuning for XGBoost\n",
    "Tuning parameters like learning_rate, n_estimators, and max_depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfa472a6-6c02-4a74-85f4-f0ee7de9e3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best XGBoost Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 300}\n",
      "Best XGBoost Score: 0.8948517265332405\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 300],    # Number of trees\n",
    "    'max_depth': [3, 5, 7],             # Tree depth\n",
    "    'learning_rate': [0.01, 0.1, 0.2]   # Learning rate\n",
    "}\n",
    "\n",
    "# Grid search for XGBoost\n",
    "grid_search_xgb = GridSearchCV(\n",
    "    estimator=XGBClassifier(random_state=42),\n",
    "    param_grid=param_grid_xgb,\n",
    "    scoring='accuracy',  # You can also use 'roc_auc'\n",
    "    cv=5,                # 5-fold cross-validation\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search on the training data\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters and the best score\n",
    "print(\"Best XGBoost Parameters:\", grid_search_xgb.best_params_)\n",
    "print(\"Best XGBoost Score:\", grid_search_xgb.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3509355-87a0-488f-beea-faabe8cb65e7",
   "metadata": {},
   "source": [
    "#### Why Perform This Tuning?\n",
    "- Random Forest: Increasing n_estimators may improve performance but can increase training time. Setting max_depth helps control overfitting for complex datasets.\n",
    "\n",
    "- XGBoost: Tuning learning_rate balances the contribution of individual trees, and n_estimators ensures sufficient iterations for optimal performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
